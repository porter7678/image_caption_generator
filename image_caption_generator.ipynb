{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_caption_generator",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/porter7678/image_caption_generator/blob/master/image_caption_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTMqietwBylh",
        "colab_type": "text"
      },
      "source": [
        "###To Do:\n",
        " - Figure out how to make captions go in, and how to do loss through the whole thing\n",
        " - Just get some outputs from the RNN\n",
        " - Figure out how to get one-hots out of the LSTM to predict the next word\n",
        "\n",
        "\n",
        "###Fix at the end:\n",
        " - Get it to work for batch size more than 1\n",
        " - Shuffle my dataloader\n",
        " - Size images back up\n",
        " - Get images to work even if theyâ€™re not square\n",
        " - I did 'drop_last' on my dataloaders, there is a chance I don't want to do that\n",
        " - If I can get the model training on just one caption, is there some way to generalize the loss to account for all 5 captions that I have? Like treat the loss as the lowest out of all 5 captions?\n",
        " - Use a different CNN that isn't copying from https://medium.com/@stepanulyanin/captioning-images-with-pytorch-bc592e5fd1a3\n",
        " - After getting something running, try to strip it down to just what the paper had (including a CNN from that year)\n",
        " - Mess with the dimensions of layers within the DenseNet to make it work better.\n",
        " - Try to use an nn.Embedding\n",
        " - Use the raw Negative Log Likelihood for loss instead of CrossEntropy\n",
        " - Use all 5 captions instead of just 1\n",
        " - Implement the LSTM from the paper\n",
        "\n",
        "###Possible improvements:\n",
        " - Different CNN\n",
        " - Implement Attention\n",
        " - Fix the embedding dictionary\n",
        " - Teacher forcing\n",
        " - Beam search\n",
        "\n",
        "###Questions\n",
        " - What does pin_memory in the DataLoader do?\n",
        " - What does .train() and .eval() do?\n",
        " - Remind me how PyTorch batches work?\n",
        " - Should I have two different optimizers for my two different models? \n",
        "\n",
        "\n",
        "\n",
        " ----------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6iebwg7rPB6",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saUiX2p428La",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install torch \n",
        "# !pip install torchvision\n",
        "# !pip install tqdm\n",
        "# !pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIPxNuxA4Bni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets, models\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "assert torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmybwgB4qRT7",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6B-whcwDICG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ['KAGGLE_USERNAME'] = 'porter7678'\n",
        "if not 'KAGGLE_KEY' in os.environ:\n",
        "  os.environ['KAGGLE_KEY'] = input()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyvIBcScVojJ",
        "colab_type": "code",
        "outputId": "d1bdbbbb-087a-4575-af29-117fddf5ef57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# ~70 secs\n",
        "!kaggle datasets download -d ming666/flicker8k-dataset\n",
        "!unzip -q flicker8k-dataset.zip\n",
        "\n",
        "# Discard extra directories\n",
        "! rm -rf flickr8k_dataset/\n",
        "! rm -rf flickr8k_text/\n",
        "! rm -rf sample_data/\n",
        "\n",
        "# Create Train and Valid directories\n",
        "os.mkdir('Flickr8k_Dataset/Train_Images')\n",
        "os.mkdir('Flickr8k_Dataset/Valid_Images')\n",
        "os.mkdir('Flickr8k_Dataset/Valid_Images/Flicker8k_Dataset_Valid')\n",
        "os.replace('Flickr8k_Dataset/Flicker8k_Dataset', 'Flickr8k_Dataset/Train_Images/Flicker8k_Dataset_Train')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading flicker8k-dataset.zip to /content\n",
            "100% 2.07G/2.08G [00:52<00:00, 41.6MB/s]\n",
            "100% 2.08G/2.08G [00:52<00:00, 42.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhyb-SX6WMT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############ - DATATSET README - ################################################################\n",
        "# Flickr8k.token.txt - the raw captions of the Flickr8k Dataset,\n",
        "#       the first column is the ID of the caption which is \"image address # caption number\"\n",
        "# Flickr8k.lemma.txt - the lemmatized version of the above captions \n",
        "# Flickr_8k.trainImages.txt - The list of training images used in our experiments\n",
        "# Flickr_8k.devImages.txt - The list of development/validation images used in our experiments\n",
        "# Flickr_8k.testImages.txt - The list of test images used in our experiments\n",
        "#################################################################################################\n",
        "# 8091 images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeX3JYPZg6Sz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create list of images to move to validation folder\n",
        "valid_img_names = []\n",
        "file_names = ['Flickr8k_text/Flickr_8k.devImages.txt', 'Flickr8k_text/Flickr_8k.testImages.txt']\n",
        "for file_name in file_names:\n",
        "    with open(file_name, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            valid_img_names.append(line.strip())\n",
        "\n",
        "# Move validation images to validation folder\n",
        "for file_name in valid_img_names:\n",
        "    os.replace('Flickr8k_Dataset/Train_Images/Flicker8k_Dataset_Train/' + file_name, 'Flickr8k_Dataset/Valid_Images/Flicker8k_Dataset_Valid/' + file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6uQvLTLqdJj",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w59wWgyifXJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageCaptionDataset(Dataset):\n",
        "    '''Dataset class containing images and captions\n",
        "\n",
        "    Attributes:\n",
        "        image_folder (ImageFolder Dataset): Images to be captioned (data)\n",
        "        caption_folder (list): Captions associated with images for validation (labels)\n",
        "    '''\n",
        "    def __init__(self, train=True, img_size=128):\n",
        "        # Initialize with training images or validation images depending on train parameter\n",
        "        folder_prefix = 'Train' if train else 'Valid'\n",
        "        folder_path = 'Flickr8k_Dataset/' + folder_prefix + '_Images'\n",
        "        self.image_folder = datasets.ImageFolder(root=folder_path,\n",
        "                                                transform=transforms.Compose([\n",
        "                                                    transforms.Resize(img_size),\n",
        "                                                    transforms.CenterCrop(img_size),  # This is the line that is making images square\n",
        "                                                    transforms.ToTensor(),\n",
        "                                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                                                ]))\n",
        "        \n",
        "        # NOTE: There is a chance I need to find a way to stick this caption attribute in a Dataset class for the dataloader to work.\n",
        "        train_img_captions = []\n",
        "        valid_img_captions = []\n",
        "        curr_img_captions = []\n",
        "\n",
        "        with open('Flickr8k_text/Flickr8k.token.txt', 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        for i, line in enumerate(lines):\n",
        "            line = line.strip().split('\\t')\n",
        "            curr_img_captions.append(line[1])\n",
        "\n",
        "            # Each image has five captions, so start a new caption list each 5 iterations\n",
        "            if i % 5 == 4:\n",
        "                img_name = line[0][:-2]\n",
        "                if img_name in valid_img_names:\n",
        "                    valid_img_captions.append((img_name, curr_img_captions))\n",
        "                else:\n",
        "                    train_img_captions.append((img_name, curr_img_captions))\n",
        "                curr_img_captions = []\n",
        "\n",
        "        # Data cleaning: This is an extra caption in the dataset with no image\n",
        "        train_img_captions.pop(1010)\n",
        "\n",
        "        self.caption_folder = train_img_captions if train else valid_img_captions\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = self.image_folder[index]\n",
        "        captions = self.caption_folder[index]\n",
        "        return img[0], captions[1][0]  # For now, I'm only taking the first of 5 captions for each image\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.caption_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAIE2WPJ3fXz",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iffRkb8185IV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size=128):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "\n",
        "        self.densenet = models.densenet121(pretrained=True)\n",
        "        self.densenet.classifier = nn.Linear(1024, 1024) # Fully connected layer\n",
        "        self.embed = nn.Linear(1024, embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, images):\n",
        "        outputs = self.relu(self.densenet(images))\n",
        "        embeddings = self.embed(outputs)\n",
        "        return embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCYJ2aRcB7MC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, n_layers=1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_word, hidden_state, cell_state, is_caption=True):\n",
        "        if is_caption:\n",
        "            embedding = self.embedding(input_word)   #.view(1, 1, -1)\n",
        "        else:\n",
        "            embedding = input_word              # This is for when we pass the image in first\n",
        "        embed = F.relu(embedding)\n",
        "        embed = embed.view(1, 1, -1)            # I'm not entirely sure why I'm doing this\n",
        "\n",
        "        output, (hidden, cell) = self.lstm(embed, (hidden_state, cell_state))\n",
        "        scores = self.out(output[0])\n",
        "\n",
        "        return scores, hidden, cell\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros((1, 1, self.hidden_size))  # NOTE: I may need to change this 1 to batch size later"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-eUnZfArCju",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM1wZ4Iygf57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_imgs(x, new_fig=True):\n",
        "    '''Displays the image contained in a tensor.\n",
        "\n",
        "    Arguments:\n",
        "        x ((c,m,n) tensor): The image to be shown\n",
        "    '''\n",
        "    grid = vutils.make_grid(x.detach().cpu(), nrow=8, normalize=True, pad_value=0.3)\n",
        "    grid = grid.transpose(0,2).transpose(0,1) # channels as last dimension\n",
        "    if new_fig:\n",
        "        plt.figure()\n",
        "    plt.imshow(grid.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe7Dxtd4pFhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_corpus_dictionary(train_dataset, valid_dataset):\n",
        "    '''Creates corpus dictionary\n",
        "    \n",
        "    Arguments:\n",
        "        train_dataset (ImageCaptionDataset): training dataset\n",
        "        valid_dataset (ImageCaptionDataset): validation dataset\n",
        "        \n",
        "    Returns:\n",
        "        caption_dictionary (list): all words in any caption (non-case sensitive), and start/end tokens\n",
        "        '''\n",
        "    # Get a dictionary going for my captions\n",
        "    caption_dictionary = []\n",
        "    max_caption_len = 0\n",
        "\n",
        "    for caption_list in train_dataset.caption_folder:\n",
        "        for caption in caption_list[1]:\n",
        "            caption = caption.split()\n",
        "            if len(caption) > max_caption_len:\n",
        "                max_caption_len = len(caption)\n",
        "            for word in caption:\n",
        "                word = word.lower()\n",
        "                if word not in caption_dictionary:\n",
        "                    caption_dictionary.append(word)\n",
        "\n",
        "    for caption_list in valid_dataset.caption_folder:\n",
        "        for caption in caption_list[1]:\n",
        "            caption = caption.split()\n",
        "            if len(caption) > max_caption_len:\n",
        "                max_caption_len = len(caption)\n",
        "            for word in caption:\n",
        "                word = word.lower()\n",
        "                if word not in caption_dictionary:\n",
        "                    caption_dictionary.append(word)\n",
        "\n",
        "    caption_dictionary.append('<SOS>')\n",
        "    caption_dictionary.append('<EOS>')\n",
        "\n",
        "    return caption_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JqVy29ROUqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_tensor(caption):\n",
        "    '''Convert caption into tensor with corresponding dictionary indices'''\n",
        "    word_tensor = torch.zeros(len(caption))\n",
        "    for i, word in enumerate(caption):\n",
        "        word_tensor[i] = caption_dictionary.index(word)\n",
        "    return word_tensor\n",
        "        \n",
        "\n",
        "def get_caption_tensors(caption):\n",
        "    '''Convert caption into input and target tensors'''\n",
        "    words = caption.lower().split()\n",
        "    # Append start and stop tokens\n",
        "    inp = ['<SOS>'] + words\n",
        "    target = words + ['<EOS>']\n",
        "\n",
        "    # Convert to tensor\n",
        "    inp = get_word_tensor(inp)\n",
        "    target = get_word_tensor(target)\n",
        "    return inp, target\n",
        "\n",
        "# Use 'torch.nn.utils.rnn.pad_sequence' to pad if necessary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTNxcEzyXet5",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vs9_E7I38uT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "outputId": "1d5308d5-cdcf-415b-95d2-ba59dcac1014"
      },
      "source": [
        "n_epochs = 1\n",
        "img_size = 128\n",
        "hidden_size = 128  # Maybe experiment with this later?\n",
        "batch_size = 1\n",
        "\n",
        "train_dataset = ImageCaptionDataset(train=True, img_size=img_size)\n",
        "valid_dataset = ImageCaptionDataset(train=False, img_size=img_size)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, \n",
        "                          batch_size=batch_size, \n",
        "                          shuffle=False, \n",
        "                          pin_memory=True,\n",
        "                          drop_last=True)\n",
        "valid_loader = DataLoader(valid_dataset, \n",
        "                          batch_size=batch_size, \n",
        "                          shuffle=False, \n",
        "                          pin_memory=True)\n",
        "\n",
        "caption_dictionary = create_corpus_dictionary(train_dataset, valid_dataset)\n",
        "vocab_size = len(caption_dictionary)\n",
        "\n",
        "encoder = EncoderCNN(embed_size=img_size).cuda()\n",
        "decoder = DecoderRNN(img_size, hidden_size, vocab_size).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "losses = []\n",
        "valid_losses = []\n",
        "\n",
        "loop = tqdm(total=len(train_loader) * n_epochs, position=0, leave=False)\n",
        "for epoch in range(n_epochs):\n",
        "    for batch, (x, captions) in enumerate(train_loader):\n",
        "        print()\n",
        "        encoder.zero_grad()\n",
        "        decoder.zero_grad()\n",
        "\n",
        "        # Get inp and label caption tensors\n",
        "        y_inp, y_truth = get_caption_tensors(captions[0]) # NOTE: This isn't gonna work when my batch size isn't 1\n",
        "        \n",
        "        # Move onto GPU\n",
        "        x = x.cuda()\n",
        "        y_inp = y_inp.cuda()\n",
        "        y_truth = y_truth.cuda()\n",
        "\n",
        "        # Encode the image\n",
        "        x_encoded = encoder(x)\n",
        "\n",
        "        # Pass in caption tensors\n",
        "        hidden_initial = decoder.init_hidden().cuda()\n",
        "        cell_initial = decoder.init_hidden().cuda()\n",
        "        output, hidden, cell = decoder(x_encoded, hidden_initial, cell_initial, is_caption=False)\n",
        "\n",
        "        # print('output', output.shape)   # 1 x 8920\n",
        "        # print('hidden', hidden.shape)   # 1 x 1 x 128\n",
        "        # print('cell', cell.shape)       # 1 x 1 x 128\n",
        "\n",
        "\n",
        "        for i, word in enumerate(y_inp):\n",
        "            output, hidden, cell = decoder(x_encoded, hidden, cell, is_caption=True)\n",
        "\n",
        "\n",
        "        break\n",
        "    break\n",
        "        # loop.update(1)\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/6091 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "tensor([[[-0.0283,  0.0646,  0.0401, -0.0004,  0.0591, -0.0467, -0.0893,\n",
            "          -0.0291,  0.0056,  0.0110,  0.0122,  0.0623, -0.0122,  0.0310,\n",
            "           0.0377, -0.0165, -0.0955, -0.0752, -0.0101,  0.0111,  0.0061,\n",
            "           0.0089, -0.0436,  0.0783,  0.0703, -0.0164, -0.0255, -0.0550,\n",
            "          -0.0106, -0.0435,  0.0280, -0.0470, -0.0023,  0.0216, -0.0127,\n",
            "          -0.0994,  0.0900, -0.0467, -0.0563,  0.0254, -0.0129,  0.0427,\n",
            "          -0.0935, -0.0685, -0.0343,  0.0133, -0.0293, -0.0197,  0.0327,\n",
            "          -0.0066, -0.0606, -0.0264, -0.0308,  0.0042,  0.0218,  0.0388,\n",
            "          -0.0323,  0.0325,  0.0417,  0.0335,  0.0133,  0.0352, -0.0702,\n",
            "          -0.0242,  0.0058,  0.0348, -0.0444,  0.0002,  0.0837, -0.0028,\n",
            "          -0.0354,  0.0436, -0.0212,  0.0242, -0.0027,  0.0865,  0.0894,\n",
            "           0.0919, -0.0067,  0.0768,  0.0289,  0.0305,  0.1046,  0.0357,\n",
            "          -0.0991, -0.0665,  0.0531, -0.0088,  0.0725,  0.0161, -0.0175,\n",
            "           0.0100, -0.0084, -0.0024,  0.0339,  0.0409, -0.0114,  0.0372,\n",
            "           0.0061, -0.0023, -0.0167,  0.0533, -0.0128, -0.0083,  0.0690,\n",
            "          -0.0145,  0.0588,  0.0412,  0.0604, -0.1168,  0.0108, -0.0582,\n",
            "          -0.0086,  0.0255, -0.0728,  0.0200, -0.0741,  0.0038, -0.0093,\n",
            "          -0.0107, -0.0191, -0.0610, -0.0983, -0.0454, -0.1026, -0.0109,\n",
            "          -0.0055,  0.0599]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcfFr7fCWv9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}