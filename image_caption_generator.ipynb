{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_caption_generator",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/porter7678/image_caption_generator/blob/master/image_caption_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTMqietwBylh",
        "colab_type": "text"
      },
      "source": [
        "###To Do:\n",
        " - Just get some outputs from the RNN\n",
        " - Decide what the forward of my LSTM will look like.\n",
        " - Fix the sizes of vectors going into my LSTM\n",
        " - Figure out how to get one-hots out of the LSTM to predict the next word\n",
        " - Try to implement the RNN part using PyTorch's built in LSTM\n",
        " - Implement the LSTM from the paper\n",
        "\n",
        "\n",
        "###Fix at the end:\n",
        " - Get it to work for batch size more than 1\n",
        " - Shuffle my dataloader\n",
        " - Size images back up\n",
        " - Get images to work even if theyâ€™re not square\n",
        " - I did 'drop_last' on my dataloaders, there is a chance I don't want to do that\n",
        " - If I can get the model training on just one caption, is there some way to generalize the loss to account for all 5 captions that I have? Like treat the loss as the lowest out of all 5 captions?\n",
        " - Use a different CNN that isn't copying from https://medium.com/@stepanulyanin/captioning-images-with-pytorch-bc592e5fd1a3\n",
        " - After getting something running, try to strip it down to just what the paper had (including a CNN from that year)\n",
        " - Mess with the dimensions of layers within the DenseNet to make it work better.\n",
        " - Try to use an nn.Embedding\n",
        " - Use the raw Negative Log Likelihood for loss instead of CrossEntropy\n",
        " - Use all 5 captions instead of just 1\n",
        "\n",
        "###Possible improvements:\n",
        " - Different CNN\n",
        " - Implement Attention\n",
        " - Fix the embedding dictionary\n",
        " - Teacher forcing\n",
        " - Beam search\n",
        "\n",
        "###Questions\n",
        " - What does pin_memory in the DataLoader do?\n",
        " - What does .train() and .eval() do?\n",
        "\n",
        "\n",
        " ----------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6iebwg7rPB6",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saUiX2p428La",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install torch \n",
        "# !pip install torchvision\n",
        "# !pip install tqdm\n",
        "# !pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIPxNuxA4Bni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets, models\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "assert torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmybwgB4qRT7",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6B-whcwDICG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ['KAGGLE_USERNAME'] = 'porter7678'\n",
        "if not 'KAGGLE_KEY' in os.environ:\n",
        "  os.environ['KAGGLE_KEY'] = input()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyvIBcScVojJ",
        "colab_type": "code",
        "outputId": "994818a2-3fd9-4e8a-d738-75df6694333a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# ~70 secs\n",
        "!kaggle datasets download -d ming666/flicker8k-dataset\n",
        "!unzip -q flicker8k-dataset.zip\n",
        "\n",
        "# Discard extra directories\n",
        "! rm -rf flickr8k_dataset/\n",
        "! rm -rf flickr8k_text/\n",
        "! rm -rf sample_data/\n",
        "\n",
        "# Create Train and Valid directories\n",
        "os.mkdir('Flickr8k_Dataset/Train_Images')\n",
        "os.mkdir('Flickr8k_Dataset/Valid_Images')\n",
        "os.mkdir('Flickr8k_Dataset/Valid_Images/Flicker8k_Dataset_Valid')\n",
        "os.replace('Flickr8k_Dataset/Flicker8k_Dataset', 'Flickr8k_Dataset/Train_Images/Flicker8k_Dataset_Train')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading flicker8k-dataset.zip to /content\n",
            " 99% 2.06G/2.08G [00:28<00:00, 44.2MB/s]\n",
            "100% 2.08G/2.08G [00:28<00:00, 77.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhyb-SX6WMT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############ - DATATSET README - ################################################################\n",
        "# Flickr8k.token.txt - the raw captions of the Flickr8k Dataset,\n",
        "#       the first column is the ID of the caption which is \"image address # caption number\"\n",
        "# Flickr8k.lemma.txt - the lemmatized version of the above captions \n",
        "# Flickr_8k.trainImages.txt - The list of training images used in our experiments\n",
        "# Flickr_8k.devImages.txt - The list of development/validation images used in our experiments\n",
        "# Flickr_8k.testImages.txt - The list of test images used in our experiments\n",
        "#################################################################################################\n",
        "# 8091 images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeX3JYPZg6Sz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create list of images to move to validation folder\n",
        "valid_img_names = []\n",
        "file_names = ['Flickr8k_text/Flickr_8k.devImages.txt', 'Flickr8k_text/Flickr_8k.testImages.txt']\n",
        "for file_name in file_names:\n",
        "    with open(file_name, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            valid_img_names.append(line.strip())\n",
        "\n",
        "# Move validation images to validation folder\n",
        "for file_name in valid_img_names:\n",
        "    os.replace('Flickr8k_Dataset/Train_Images/Flicker8k_Dataset_Train/' + file_name, 'Flickr8k_Dataset/Valid_Images/Flicker8k_Dataset_Valid/' + file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6uQvLTLqdJj",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w59wWgyifXJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageCaptionDataset(Dataset):\n",
        "    '''Dataset class containing images and captions\n",
        "\n",
        "    Attributes:\n",
        "        image_folder (ImageFolder Dataset): Images to be captioned (data)\n",
        "        caption_folder (list): Captions associated with images for validation (labels)\n",
        "    '''\n",
        "    def __init__(self, train=True, img_size=128):\n",
        "        # Initialize with training images or validation images depending on train parameter\n",
        "        folder_prefix = 'Train' if train else 'Valid'\n",
        "        folder_path = 'Flickr8k_Dataset/' + folder_prefix + '_Images'\n",
        "        self.image_folder = datasets.ImageFolder(root=folder_path,\n",
        "                                                transform=transforms.Compose([\n",
        "                                                    transforms.Resize(img_size),\n",
        "                                                    transforms.CenterCrop(img_size),  # This is the line that is making images square\n",
        "                                                    transforms.ToTensor(),\n",
        "                                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                                                ]))\n",
        "        \n",
        "        # NOTE: There is a chance I need to find a way to stick this caption attribute in a Dataset class for the dataloader to work.\n",
        "        train_img_captions = []\n",
        "        valid_img_captions = []\n",
        "        curr_img_captions = []\n",
        "\n",
        "        with open('Flickr8k_text/Flickr8k.token.txt', 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        for i, line in enumerate(lines):\n",
        "            line = line.strip().split('\\t')\n",
        "            curr_img_captions.append(line[1])\n",
        "\n",
        "            # Each image has five captions, so start a new caption list each 5 iterations\n",
        "            if i % 5 == 4:\n",
        "                img_name = line[0][:-2]\n",
        "                if img_name in valid_img_names:\n",
        "                    valid_img_captions.append((img_name, curr_img_captions))\n",
        "                else:\n",
        "                    train_img_captions.append((img_name, curr_img_captions))\n",
        "                curr_img_captions = []\n",
        "\n",
        "        # Data cleaning: This is an extra caption in the dataset with no image\n",
        "        train_img_captions.pop(1010)\n",
        "\n",
        "        self.caption_folder = train_img_captions if train else valid_img_captions\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = self.image_folder[index]\n",
        "        captions = self.caption_folder[index]\n",
        "        return img[0], captions[1][0]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.caption_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAIE2WPJ3fXz",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iffRkb8185IV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size=128):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "\n",
        "        self.densenet = models.densenet121(pretrained=True)\n",
        "        self.densenet.classifier = nn.Linear(1024, 1024) # Fully connected layer\n",
        "        self.embed = nn.Linear(1024, embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, images):\n",
        "        outputs = self.relu(self.densenet(images))\n",
        "        embeddings = self.embed(outputs)\n",
        "        return embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCYJ2aRcB7MC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, n_layers=1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_word, hidden, is_caption=True):\n",
        "        print('yeet')\n",
        "        print(input_word.dtype)\n",
        "        print(input_word)\n",
        "        if is_caption:\n",
        "            embedding = self.embedding(input_word)          #.view(1, 1, -1)\n",
        "        else:\n",
        "            embedding = input_word # In the case of\n",
        "        embed = F.relu(embedding)\n",
        "        output, hidden = self.lstm(embed, hidden)\n",
        "        scores = self.out(output[0])\n",
        "\n",
        "        return scores, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-eUnZfArCju",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM1wZ4Iygf57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_imgs(x, new_fig=True):\n",
        "    '''Displays the image contained in a tensor.\n",
        "\n",
        "    Arguments:\n",
        "        x ((c,m,n) tensor): The image to be shown\n",
        "    '''\n",
        "    grid = vutils.make_grid(x.detach().cpu(), nrow=8, normalize=True, pad_value=0.3)\n",
        "    grid = grid.transpose(0,2).transpose(0,1) # channels as last dimension\n",
        "    if new_fig:\n",
        "        plt.figure()\n",
        "    plt.imshow(grid.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe7Dxtd4pFhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_corpus_dictionary(train_dataset, valid_dataset):\n",
        "    '''Creates corpus dictionary\n",
        "    \n",
        "    Arguments:\n",
        "        train_dataset (ImageCaptionDataset): training dataset\n",
        "        valid_dataset (ImageCaptionDataset): validation dataset\n",
        "        \n",
        "    Returns:\n",
        "        caption_dictionary (list): all words in any caption (non-case sensitive), and start/end tokens\n",
        "        '''\n",
        "    # Get a dictionary going for my captions\n",
        "    caption_dictionary = []\n",
        "    max_caption_len = 0\n",
        "\n",
        "    for caption_list in train_dataset.caption_folder:\n",
        "        for caption in caption_list[1]:\n",
        "            caption = caption.split()\n",
        "            if len(caption) > max_caption_len:\n",
        "                max_caption_len = len(caption)\n",
        "            for word in caption:\n",
        "                word = word.lower()\n",
        "                if word not in caption_dictionary:\n",
        "                    caption_dictionary.append(word)\n",
        "\n",
        "    for caption_list in valid_dataset.caption_folder:\n",
        "        for caption in caption_list[1]:\n",
        "            caption = caption.split()\n",
        "            if len(caption) > max_caption_len:\n",
        "                max_caption_len = len(caption)\n",
        "            for word in caption:\n",
        "                word = word.lower()\n",
        "                if word not in caption_dictionary:\n",
        "                    caption_dictionary.append(word)\n",
        "\n",
        "    caption_dictionary.append('<SOS>')\n",
        "    caption_dictionary.append('<EOS>')\n",
        "\n",
        "    return caption_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JqVy29ROUqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_tensor(caption):\n",
        "    '''Convert caption into tensor with corresponding dictionary indices'''\n",
        "    word_tensor = torch.zeros(len(caption))\n",
        "    for i, word in enumerate(caption):\n",
        "        word_tensor[i] = caption_dictionary.index(word)\n",
        "    return word_tensor\n",
        "        \n",
        "\n",
        "def get_caption_tensors(caption):\n",
        "    '''Convert caption into input and target tensors'''\n",
        "    words = caption.lower().split()\n",
        "    # Append start and stop tokens\n",
        "    inp = ['<SOS>'] + words\n",
        "    target = words + ['<EOS>']\n",
        "\n",
        "    # Convert to tensor\n",
        "    inp = get_word_tensor(inp)\n",
        "    target = get_word_tensor(target)\n",
        "    return inp, target\n",
        "\n",
        "# Use 'torch.nn.utils.rnn.pad_sequence' to pad if necessary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTNxcEzyXet5",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vs9_E7I38uT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "8c6482b8-58fe-4b2e-c89d-eff6f3c88565"
      },
      "source": [
        "n_epochs = 1\n",
        "img_size = 128\n",
        "hidden_size = 128  # Maybe experiment with this later?\n",
        "batch_size = 1\n",
        "\n",
        "train_dataset = ImageCaptionDataset(train=True, img_size=img_size)\n",
        "valid_dataset = ImageCaptionDataset(train=False, img_size=img_size)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, \n",
        "                          batch_size=batch_size, \n",
        "                          shuffle=False, \n",
        "                          pin_memory=True,\n",
        "                          drop_last=True)\n",
        "valid_loader = DataLoader(valid_dataset, \n",
        "                          batch_size=batch_size, \n",
        "                          shuffle=False, \n",
        "                          pin_memory=True)\n",
        "\n",
        "caption_dictionary = create_corpus_dictionary(train_dataset, valid_dataset)\n",
        "vocab_size = len(caption_dictionary)\n",
        "\n",
        "encoder = EncoderCNN(embed_size=img_size).cuda()\n",
        "decoder = DecoderRNN(img_size, hidden_size, vocab_size).cuda()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "losses = []\n",
        "valid_losses = []\n",
        "\n",
        "loop = tqdm(total=len(train_loader) * n_epochs, position=0, leave=False)\n",
        "for epoch in range(n_epochs):\n",
        "    for batch, (x, captions) in enumerate(train_loader):\n",
        "        print()\n",
        "        encoder.zero_grad()\n",
        "        decoder.zero_grad()\n",
        "        \n",
        "        # Get inp and label caption tensors\n",
        "        y_inp, y_truth = get_caption_tensors(captions[0]) # NOTE: This isn't gonna work when my batch size isn't 1\n",
        "        \n",
        "        # Move onto GPU\n",
        "        x = x.cuda()\n",
        "        y_inp = y_inp.cuda()\n",
        "        y_truth = y_truth.cuda()\n",
        "\n",
        "        # Encode the image\n",
        "        x_encoded = encoder(x)\n",
        "\n",
        "        # Pass in caption tensors\n",
        "        y_hat = decoder(x_encoded, y_inp)\n",
        "        print(y_hat.shape)\n",
        "\n",
        "        break\n",
        "    break\n",
        "        # loop.update(1)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/6091 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "yeet\n",
            "torch.float32\n",
            "tensor([[ 0.3705, -0.3601, -0.0884,  0.1953, -0.1273, -0.1514, -0.1324,  0.0080,\n",
            "          0.1235,  0.1189,  0.0864,  0.2961,  0.0040, -0.0851, -0.1489,  0.0432,\n",
            "         -0.2004, -0.0160,  0.0766, -0.1082,  0.0076, -0.2786, -0.2475,  0.1523,\n",
            "         -0.1607, -0.1910, -0.2432, -0.0995, -0.1581, -0.1246, -0.0178,  0.3051,\n",
            "         -0.2917,  0.2712,  0.0985, -0.1212,  0.2364,  0.0305, -0.1877,  0.1478,\n",
            "         -0.0439, -0.0350,  0.1022,  0.0369, -0.2351,  0.1190,  0.0573,  0.1671,\n",
            "         -0.0646, -0.3463, -0.1344, -0.0122,  0.0220, -0.1302,  0.0975,  0.1024,\n",
            "         -0.2532, -0.1122,  0.1835,  0.0213, -0.0597, -0.2618,  0.0090,  0.0442,\n",
            "         -0.1930,  0.1137, -0.1206, -0.1629,  0.1575,  0.0686,  0.0106, -0.0845,\n",
            "         -0.1306,  0.2260,  0.2362, -0.1066,  0.2373, -0.0491,  0.1062, -0.0062,\n",
            "         -0.0821,  0.0523, -0.1958,  0.2545,  0.0687, -0.0370,  0.1863, -0.0670,\n",
            "          0.0847,  0.1089, -0.1009,  0.1204,  0.2039,  0.0907,  0.0609, -0.1532,\n",
            "         -0.0676,  0.0726, -0.1344,  0.2048,  0.1155,  0.0567, -0.1173, -0.0369,\n",
            "          0.0224, -0.0672,  0.0524,  0.0788, -0.2131,  0.2411, -0.2132, -0.0186,\n",
            "         -0.1167, -0.2865, -0.0922,  0.1845,  0.0519, -0.2489,  0.1479,  0.0206,\n",
            "         -0.1550,  0.0535,  0.2220,  0.0445, -0.1534, -0.4131,  0.0011,  0.2175]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-bb0b11978bc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Pass in caption tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-0878fc3118c0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_word, hidden)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_word\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m#.view(1, 1, -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yah'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have scalar type Long; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcfFr7fCWv9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}