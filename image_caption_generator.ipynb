{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_caption_generator",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/porter7678/image_caption_generator/blob/master/image_caption_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTMqietwBylh",
        "colab_type": "text"
      },
      "source": [
        "###To Do:\n",
        " - Decide what the forward of my LSTM will look like.\n",
        " - Fix the sizes of vectors going into my LSTM\n",
        " - Figure out how to get one-hots out of the LSTM to predict the next word\n",
        " - Try to implement the RNN part using PyTorch's built in LSTM\n",
        " - Implement the LSTM from the paper\n",
        "\n",
        "\n",
        "###Fix at the end:\n",
        " - Get it to work for batch size more than 1\n",
        " - Shuffle my dataloader\n",
        " - Size images back up\n",
        " - Get images to work even if theyâ€™re not square\n",
        " - I did 'drop_last' on my dataloaders, there is a chance I don't want to do that\n",
        " - If I can get the model training on just one caption, is there some way to generalize the loss to account for all 5 captions that I have? Like treat the loss as the lowest out of all 5 captions?\n",
        " - Use a different CNN that isn't copying from https://medium.com/@stepanulyanin/captioning-images-with-pytorch-bc592e5fd1a3\n",
        " - After getting something running, try to strip it down to just what the paper had (including a CNN from that year)\n",
        " - Mess with the dimensions of layers within the DenseNet to make it work better.\n",
        " - Try to use an nn.Embedding\n",
        " - Use the raw Negative Log Likelihood for loss instead of CrossEntropy\n",
        " - Use all 5 captions instead of just 1\n",
        "\n",
        "###Possible improvements:\n",
        " - Different CNN\n",
        " - Implement Attention\n",
        " - Fix the embedding dictionary\n",
        " - Teacher forcing\n",
        " - Beam search\n",
        "\n",
        "###Questions\n",
        " - What does pin_memory in the DataLoader do?\n",
        " - What does .train() and .eval() do?\n",
        "\n",
        " ----------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6iebwg7rPB6",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saUiX2p428La",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install torch \n",
        "# !pip install torchvision\n",
        "# !pip install tqdm\n",
        "# !pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIPxNuxA4Bni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, utils, datasets, models\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "assert torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmybwgB4qRT7",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6B-whcwDICG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.environ['KAGGLE_USERNAME'] = 'porter7678'\n",
        "if not 'KAGGLE_KEY' in os.environ:\n",
        "  os.environ['KAGGLE_KEY'] = input()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyvIBcScVojJ",
        "colab_type": "code",
        "outputId": "37664c53-0581-4a77-963a-d2fbe09d86c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# ~70 secs\n",
        "!kaggle datasets download -d ming666/flicker8k-dataset\n",
        "!unzip -q flicker8k-dataset.zip\n",
        "\n",
        "# Discard extra directories\n",
        "! rm -rf flickr8k_dataset/\n",
        "! rm -rf flickr8k_text/\n",
        "! rm -rf sample_data/\n",
        "\n",
        "# Create Train and Valid directories\n",
        "os.mkdir('Flickr8k_Dataset/Train_Images')\n",
        "os.mkdir('Flickr8k_Dataset/Valid_Images')\n",
        "os.mkdir('Flickr8k_Dataset/Valid_Images/Flicker8k_Dataset_Valid')\n",
        "os.replace('Flickr8k_Dataset/Flicker8k_Dataset', 'Flickr8k_Dataset/Train_Images/Flicker8k_Dataset_Train')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading flicker8k-dataset.zip to /content\n",
            "100% 2.07G/2.08G [00:26<00:00, 77.8MB/s]\n",
            "100% 2.08G/2.08G [00:26<00:00, 83.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhyb-SX6WMT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############ - DATATSET README - ################################################################\n",
        "# Flickr8k.token.txt - the raw captions of the Flickr8k Dataset,\n",
        "#       the first column is the ID of the caption which is \"image address # caption number\"\n",
        "# Flickr8k.lemma.txt - the lemmatized version of the above captions \n",
        "# Flickr_8k.trainImages.txt - The list of training images used in our experiments\n",
        "# Flickr_8k.devImages.txt - The list of development/validation images used in our experiments\n",
        "# Flickr_8k.testImages.txt - The list of test images used in our experiments\n",
        "#################################################################################################\n",
        "# 8091 images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeX3JYPZg6Sz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create list of images to move to validation folder\n",
        "valid_img_names = []\n",
        "file_names = ['Flickr8k_text/Flickr_8k.devImages.txt', 'Flickr8k_text/Flickr_8k.testImages.txt']\n",
        "for file_name in file_names:\n",
        "    with open(file_name, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            valid_img_names.append(line.strip())\n",
        "\n",
        "# Move validation images to validation folder\n",
        "for file_name in valid_img_names:\n",
        "    os.replace('Flickr8k_Dataset/Train_Images/Flicker8k_Dataset_Train/' + file_name, 'Flickr8k_Dataset/Valid_Images/Flicker8k_Dataset_Valid/' + file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6uQvLTLqdJj",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w59wWgyifXJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ImageCaptionDataset(Dataset):\n",
        "    '''Dataset class containing images and captions\n",
        "\n",
        "    Attributes:\n",
        "        image_folder (ImageFolder Dataset): Images to be captioned (data)\n",
        "        caption_folder (list): Captions associated with images for validation (labels)\n",
        "    '''\n",
        "    def __init__(self, train=True, img_size=128):\n",
        "        # Initialize with training images or validation images depending on train parameter\n",
        "        folder_prefix = 'Train' if train else 'Valid'\n",
        "        folder_path = 'Flickr8k_Dataset/' + folder_prefix + '_Images'\n",
        "        self.image_folder = datasets.ImageFolder(root=folder_path,\n",
        "                                                transform=transforms.Compose([\n",
        "                                                    transforms.Resize(img_size),\n",
        "                                                    transforms.CenterCrop(img_size),  # This is the line that is making images square\n",
        "                                                    transforms.ToTensor(),\n",
        "                                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                                                ]))\n",
        "        \n",
        "        # NOTE: There is a chance I need to find a way to stick this caption attribute in a Dataset class for the dataloader to work.\n",
        "        train_img_captions = []\n",
        "        valid_img_captions = []\n",
        "        curr_img_captions = []\n",
        "\n",
        "        with open('Flickr8k_text/Flickr8k.token.txt', 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        for i, line in enumerate(lines):\n",
        "            line = line.strip().split('\\t')\n",
        "            curr_img_captions.append(line[1])\n",
        "\n",
        "            # Each image has five captions, so start a new caption list each 5 iterations\n",
        "            if i % 5 == 4:\n",
        "                img_name = line[0][:-2]\n",
        "                if img_name in valid_img_names:\n",
        "                    valid_img_captions.append((img_name, curr_img_captions))\n",
        "                else:\n",
        "                    train_img_captions.append((img_name, curr_img_captions))\n",
        "                curr_img_captions = []\n",
        "\n",
        "        # Data cleaning: This is an extra caption in the dataset with no image\n",
        "        train_img_captions.pop(1010)\n",
        "\n",
        "        self.caption_folder = train_img_captions if train else valid_img_captions\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        img = self.image_folder[index]\n",
        "        captions = self.caption_folder[index]\n",
        "        return img[0], captions[1][0]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.caption_folder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAIE2WPJ3fXz",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iffRkb8185IV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# My output size of 128 is completely arbitrary - I don't know what I should pick\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size=128):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "\n",
        "        self.densenet = models.densenet121(pretrained=True)\n",
        "        self.densenet.classifier = nn.Linear(128, 128) # Fully connected layer\n",
        "        self.embed = nn.Linear(128, embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, images):\n",
        "        outputs = self.relu(self.densenet(images))\n",
        "        embeddings = self.embed(outputs)\n",
        "        return embeddings\n",
        "\n",
        "encoder = EncoderCNN()\n",
        "encoder = encoder.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCYJ2aRcB7MC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, n_layers=1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_word, hidden):\n",
        "        print('yeet')\n",
        "        embedding = self.embedding(input_word).view(1, 1, -1)\n",
        "        print('yah')\n",
        "        embed = F.relu(embedding)\n",
        "        output, hidden = self.lstm(embed, hidden)\n",
        "        scores = self.out(output[0])\n",
        "\n",
        "        return scores, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-eUnZfArCju",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM1wZ4Iygf57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_imgs(x, new_fig=True):\n",
        "    '''Displays the image contained in a tensor.\n",
        "\n",
        "    Arguments:\n",
        "        x ((c,m,n) tensor): The image to be shown\n",
        "    '''\n",
        "    grid = vutils.make_grid(x.detach().cpu(), nrow=8, normalize=True, pad_value=0.3)\n",
        "    grid = grid.transpose(0,2).transpose(0,1) # channels as last dimension\n",
        "    if new_fig:\n",
        "        plt.figure()\n",
        "    plt.imshow(grid.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe7Dxtd4pFhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_corpus_dictionary(train_dataset, valid_dataset):\n",
        "    '''Creates corpus dictionary\n",
        "    \n",
        "    Arguments:\n",
        "        train_dataset (ImageCaptionDataset): training dataset\n",
        "        valid_dataset (ImageCaptionDataset): validation dataset\n",
        "        \n",
        "    Returns:\n",
        "        caption_dictionary (list): all words in any caption (non-case sensitive), and start/end tokens\n",
        "        '''\n",
        "    # Get a dictionary going for my captions\n",
        "    caption_dictionary = []\n",
        "    max_caption_len = 0\n",
        "\n",
        "    for caption_list in train_dataset.caption_folder:\n",
        "        for caption in caption_list[1]:\n",
        "            caption = caption.split()\n",
        "            if len(caption) > max_caption_len:\n",
        "                max_caption_len = len(caption)\n",
        "            for word in caption:\n",
        "                word = word.lower()\n",
        "                if word not in caption_dictionary:\n",
        "                    caption_dictionary.append(word)\n",
        "\n",
        "    for caption_list in valid_dataset.caption_folder:\n",
        "        for caption in caption_list[1]:\n",
        "            caption = caption.split()\n",
        "            if len(caption) > max_caption_len:\n",
        "                max_caption_len = len(caption)\n",
        "            for word in caption:\n",
        "                word = word.lower()\n",
        "                if word not in caption_dictionary:\n",
        "                    caption_dictionary.append(word)\n",
        "\n",
        "    caption_dictionary.append('<SOS>')\n",
        "    caption_dictionary.append('<EOS>')\n",
        "\n",
        "    return caption_dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JqVy29ROUqU",
        "colab_type": "code",
        "outputId": "8fc4ec87-e760-468f-b888-ee5116caef24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "def get_word_tensor(caption):\n",
        "    '''Convert caption into tensor with corresponding dictionary indices'''\n",
        "    word_tensor = torch.zeros(len(caption))\n",
        "    for i, word in enumerate(caption):\n",
        "        word_tensor[i] = caption_dictionary.index(word)\n",
        "    return word_tensor\n",
        "        \n",
        "\n",
        "def get_caption_tensors(caption):\n",
        "    '''Convert caption into input and target tensors'''\n",
        "    print(repr(caption))\n",
        "    \n",
        "    words = caption.lower().split()\n",
        "    # Append start and stop tokens\n",
        "    inp = ['<SOS>'] + words\n",
        "    target = words + ['<EOS>']\n",
        "\n",
        "    # Convert to tensor\n",
        "    inp = get_word_tensor(inp)\n",
        "    target = get_word_tensor(target)\n",
        "    return inp, target\n",
        "\n",
        "my_caption = train_dataset[49][1]\n",
        "inp, target = get_caption_tensors(my_caption)\n",
        "\n",
        "print(inp)\n",
        "print(target)\n",
        "\n",
        "# Use 'torch.nn.utils.rnn.pad_sequence' to pad if necessary"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'a boy cleans the bubbles off his face .'\n",
            "tensor([8918.,    0.,  146.,  499.,   22.,  500.,  140.,   78.,  202.,   14.])\n",
            "tensor([   0.,  146.,  499.,   22.,  500.,  140.,   78.,  202.,   14., 8919.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTNxcEzyXet5",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vs9_E7I38uT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "f2fc912c-577d-404e-9912-ba18d1f294f8"
      },
      "source": [
        "n_epochs = 1\n",
        "img_size = 128\n",
        "hidden_size = 128  # Maybe experiment with this later?\n",
        "batch_size = 1\n",
        "\n",
        "train_dataset = ImageCaptionDataset(train=True, img_size=img_size)\n",
        "valid_dataset = ImageCaptionDataset(train=False, img_size=img_size)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, \n",
        "                          batch_size=batch_size, \n",
        "                          shuffle=False, \n",
        "                          pin_memory=True,\n",
        "                          drop_last=True)\n",
        "valid_loader = DataLoader(valid_dataset, \n",
        "                          batch_size=batch_size, \n",
        "                          shuffle=False, \n",
        "                          pin_memory=True)\n",
        "\n",
        "caption_dictionary = create_corpus_dictionary(train_dataset, valid_dataset)\n",
        "vocab_size = len(caption_dictionary)\n",
        "\n",
        "encoder = EncoderCNN(embed_size=img_size)\n",
        "decoder = DecoderRNN(img_size, hidden_size, vocab_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "losses = []\n",
        "valid_losses = []\n",
        "\n",
        "loop = tqdm(total=len(train_loader) * n_epochs, position=0, leave=False)\n",
        "for epoch in range(n_epochs):\n",
        "    for batch, (x, captions) in enumerate(train_loader):\n",
        "        print()\n",
        "        encoder.zero_grad()\n",
        "        decoder.zero_grad()\n",
        "        \n",
        "        # Get inp and label caption tensors\n",
        "        y_inp, y_truth = get_caption_tensors(captions[0]) # NOTE: This isn't gonna work when my batch size isn't 1\n",
        "        \n",
        "        # Move onto GPU\n",
        "        x = x.cuda()\n",
        "        y_inp = y_inp.cuda()\n",
        "        y_truth = y_truth.cuda()\n",
        "\n",
        "        x_encoded = EncoderCNN(x)\n",
        "        \n",
        "        # y_hat = DecoderRNN(x_encoded, y_inp)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        break\n",
        "    break\n",
        "        # loop.update(1)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/6091 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "'A child in a pink dress is climbing up a set of stairs in an entry way .'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-b88779a9daa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0my_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_truth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mx_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# y_hat = DecoderRNN(x_encoded, y_inp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-b6bf76abf7b0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensenet121\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensenet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Fully connected layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (Tensor, int), but expected one of:\n * (torch.device device)\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, torch.device device)\n      didn't match because some of the arguments have invalid types: (!Tensor!, !int!)\n * (object data, torch.device device)\n      didn't match because some of the arguments have invalid types: (!Tensor!, !int!)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcfFr7fCWv9T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}